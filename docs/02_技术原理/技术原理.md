# 2. 技术原理

本章节深入介绍 Aura Alpha 的核心技术原理，帮助开发者理解系统的工作机制。

---

## 🎯 系统概述

Aura Alpha 是一款集成了语音交互、视觉感知和运动控制的智能机器人平台。系统基于 ROS2 构建，采用分层架构设计，实现了模块化和高可扩展性。

### 核心能力

| 能力 | 技术方案 | 说明 |
|------|---------|------|
| 语音交互 | 云端实时 AI | 支持多家云服务商的语音识别和合成 |
| 人体感知 | BPU 加速推理 | 基于 YOLO Pose 的人体检测和关键点识别 |
| 运动控制 | 差速驱动 | 串口通信控制双轮差速底盘 |
| 状态反馈 | 多模态显示 | 根据 AI 状态切换动画视频 |

---

## 🎤 语音交互原理

### 整体流程

```
┌─────────┐    PCM     ┌─────────────┐   WebSocket   ┌─────────┐
│ 麦克风   │ ────────→ │ 音频节点     │ ────────────→ │ 云端 AI  │
│ WM8960  │           │ audio_node  │              │ Server  │
└─────────┘           └─────────────┘              └────┬────┘
                                                       │
┌─────────┐    PCM     ┌─────────────┐    TTS 音频    │
│ 扬声器   │ ←──────── │ 音频节点     │ ←─────────────┘
│ WM8960  │           │ audio_node  │
└─────────┘           └─────────────┘
```

### 音频采集

**硬件**：WM8960 音频编解码芯片

**采集参数**：
- 采样率：16000 Hz
- 位深度：16 bit
- 通道数：2（双声道）
- 帧大小：320 采样点（20ms）

**数据流**：
1. WM8960 通过 I2S 接口采集模拟音频
2. ALSA 驱动将音频数据转换为 PCM 格式
3. 音频节点以 50Hz 频率发布到 `/audio_data` 话题

### 云端 AI 交互

**通信协议**：WebSocket 全双工通信

**支持的服务商**：

| 服务商 | 特点 | 延迟 |
|--------|------|------|
| 火山引擎 | 国内低延迟，中文优化 | < 500ms |
| 百度 | 国内服务，稳定可靠 | < 600ms |
| OpenAI | GPT-4o 实时 API | < 800ms |
| Gemini | Google 多模态 AI | < 700ms |

**状态机**：

```
         ┌──────────────────────────────────────┐
         │                                      │
         ▼                                      │
    ┌─────────┐    检测到语音    ┌───────────┐  │
    │  idle   │ ──────────────→ │ listening │  │
    │  空闲   │                 │   监听    │  │
    └─────────┘                 └─────┬─────┘  │
         ▲                           │        │
         │                      语音结束       │
         │                           │        │
         │                           ▼        │
    ┌─────────┐    TTS 完成     ┌───────────┐  │
    │speaking │ ←────────────── │ thinking  │  │
    │  说话   │                 │   思考    │  │
    └────┬────┘                 └───────────┘  │
         │                                      │
         └──────────────────────────────────────┘
```

### 语音活动检测 (VAD)

系统使用能量阈值法进行语音活动检测：

```python
# 伪代码
def detect_voice_activity(audio_frame):
    energy = calculate_rms(audio_frame)
    if energy > VOICE_THRESHOLD:
        return True  # 检测到语音
    return False
```

**参数**：
- 能量阈值：动态自适应
- 静音超时：800ms（判定语音结束）
- 最小语音长度：300ms（过滤噪声）

---

## 👁️ 视觉感知原理

### 人体检测

**模型**：YOLOv8n Pose

**输入**：640×640 NV12 图像

**输出**：
- 人体边界框（Bounding Box）
- 17 个人体关键点坐标
- 置信度分数

**推理加速**：RDK X5 BPU（神经网络处理单元）

### 关键点定义

```
        0 (鼻子)
       / \
      1   2 (眼睛)
     /     \
    3       4 (耳朵)

    5 ───── 6 (肩膀)
    │       │
    7       8 (肘部)
    │       │
    9      10 (手腕)

   11 ───── 12 (髋部)
    │       │
   13      14 (膝盖)
    │       │
   15      16 (脚踝)
```

### 人体跟踪算法

**目标选择**：选择画面中面积最大的人体作为跟踪目标

**跟踪策略**：

```python
# 伪代码
def calculate_control(target_bbox, image_width, image_height):
    # 计算目标中心偏移
    center_x = (target_bbox.x + target_bbox.width / 2) / image_width
    offset_x = center_x - 0.5  # 相对于画面中心的偏移

    # 计算目标距离（基于边界框高度）
    target_height_ratio = target_bbox.height / image_height

    # 生成控制指令
    angular_z = -offset_x * ANGULAR_GAIN  # 角速度（转向）
    linear_x = (TARGET_HEIGHT_RATIO - target_height_ratio) * LINEAR_GAIN  # 线速度（前进/后退）

    return linear_x, angular_z
```

**控制参数**：
- 目标高度比：0.4（目标人体占画面高度的 40%）
- 死区范围：0.1（避免小幅抖动）
- 丢失阈值：30 帧（约 1 秒后停止跟踪）

---

## 🚗 运动控制原理

### 差速驱动模型

Aura Alpha 采用双轮差速驱动底盘：

```
        前进方向
           ↑
    ┌──────┴──────┐
    │             │
   [L]           [R]
  左轮           右轮
    │             │
    └──────┬──────┘
           │
        wheel_base
```

**运动学方程**：

$$
v_{left} = v_x - \omega_z \times \frac{wheel\_base}{2}
$$

$$
v_{right} = v_x + \omega_z \times \frac{wheel\_base}{2}
$$

其中：
- $v_x$：线速度（m/s）
- $\omega_z$：角速度（rad/s）
- $wheel\_base$：轴距（m）

### 速度约束

| 参数 | 默认值 | 单位 | 说明 |
|------|--------|------|------|
| 最大线速度 | 0.5 | m/s | 前进/后退速度上限 |
| 最大角速度 | 2.0 | rad/s | 旋转速度上限 |
| 轴距 | 0.30 | m | 左右轮中心距离 |

### 安全机制

**超时停车**：
- 若 500ms 内未收到新的速度指令，自动停车
- 防止通信中断导致失控

**速度平滑**：
- 加速度限制，避免急加速/急减速
- 提高运动平稳性

**串口重连**：
- 检测到串口断开后，每 3 秒尝试重连
- 发布连接状态到 `/motor/connected`

---

## 📡 传感器原理

### IMU（惯性测量单元）

**规格**：六轴 IMU（三轴陀螺仪 + 三轴加速度计）

**数据输出**：
- 角速度：$(\omega_x, \omega_y, \omega_z)$，单位 rad/s
- 姿态四元数：$(q_w, q_x, q_y, q_z)$

**采样频率**：10 Hz

**用途**：
- 姿态估计
- 运动状态监测
- 碰撞检测

### 电池监控

**监测参数**：
- 电压（V）
- 电量百分比（%）

**低电量保护**：
- 电量 < 20%：发出警告
- 电量 < 10%：限制最大速度
- 电量 < 5%：自动关机

---

## 🔊 音频处理原理

### WM8960 芯片架构

```
┌─────────────────────────────────────────────────┐
│                   WM8960                        │
│  ┌─────────┐    ┌─────────┐    ┌─────────────┐  │
│  │ 麦克风   │───→│  ADC    │───→│   I2S 输出   │──→ CPU
│  │ 输入    │    │ 模数转换 │    │             │  │
│  └─────────┘    └─────────┘    └─────────────┘  │
│                                                 │
│  ┌─────────┐    ┌─────────┐    ┌─────────────┐  │
│  │ 扬声器   │←───│  DAC    │←───│   I2S 输入   │←── CPU
│  │ 输出    │    │ 数模转换 │    │             │  │
│  └─────────┘    └─────────┘    └─────────────┘  │
└─────────────────────────────────────────────────┘
```

### 音量控制

**录音增益链**：

```
麦克风 → 输入增益(0-7) → ADC → 录音音量(0-63) → PCM 音量(0-255)
```

**播放增益链**：

```
PCM 音量(0-255) → DAC → 扬声器音量(0-127) → DC/AC 增益(0-5) → 扬声器
```

### 双工模式

系统支持同时录音和播放（全双工），通过独立的线程处理：

```python
# 录音线程
def record_thread():
    while running:
        audio_data = alsa_read(period_size)
        publish_to_topic(audio_data)

# 播放线程
def playback_thread():
    while running:
        audio_data = get_from_queue()
        alsa_write(audio_data)
```

**缓冲策略**：
- 录音队列：100 帧（约 2 秒）
- 播放队列：500 帧（约 10 秒）

---

## 🖥️ 显示原理

### 状态映射

显示节点根据 AI 状态切换对应的动画视频：

| AI 状态 | 视频文件 | 动画效果 |
|---------|---------|---------|
| `idle` | idle.mp4 | 待机呼吸动画 |
| `listening` | listening.mp4 | 聆听波形动画 |
| `thinking` | thinking.mp4 | 思考旋转动画 |
| `speaking` | speaking.mp4 | 说话嘴型动画 |
| `error` | error.mp4 | 错误提示动画 |

### 渲染后端

| 后端 | 优点 | 缺点 | 适用场景 |
|------|------|------|---------|
| GStreamer | 硬件加速，低 CPU | 配置复杂 | 生产环境 |
| Pygame | 简单易用 | CPU 占用高 | 开发调试 |
| Tkinter | 轻量级 | 性能一般 | 简单场景 |

### 视频预加载

为减少切换延迟，系统启动时预加载所有视频到内存：

```python
def preload_videos():
    videos = {}
    for state in ['idle', 'listening', 'thinking', 'speaking', 'error']:
        videos[state] = load_video(f'{state}.mp4')
    return videos
```

---

## 🔗 通信机制

### ROS2 话题通信

**QoS 配置**：

| 话题类型 | 可靠性 | 历史 | 深度 |
|---------|--------|------|------|
| 音频数据 | Best Effort | Keep Last | 10 |
| 控制指令 | Reliable | Keep Last | 10 |
| 状态信息 | Reliable | Keep Last | 1 |
| 传感器数据 | Best Effort | Keep Last | 10 |

### 节点同步

系统使用话题信号实现节点间同步：

```
display_node 启动完成
    │
    └──→ 发布 /display/ready = true
              │
              └──→ cloud_realtime_node 开始连接云端
                        │
                        └──→ 发布 /cloud_realtime/ready = true
                                  │
                                  └──→ body_tracking_node 开始工作
```

**超时机制**：
- 等待超时：30 秒
- 超时后：跳过依赖，继续启动

---

## ⚡ 性能优化

### BPU 加速

RDK X5 的 BPU 提供神经网络推理加速：

| 指标 | CPU 推理 | BPU 推理 |
|------|---------|---------|
| 推理延迟 | ~200ms | ~30ms |
| CPU 占用 | ~80% | ~5% |
| 功耗 | 高 | 低 |

### 共享内存

MIPI 相机使用共享内存传输图像，避免数据拷贝：

```
相机 → 共享内存 → 检测节点
         ↑
      零拷贝
```

### 异步处理

关键路径采用异步处理，避免阻塞：

- 音频采集/播放：独立线程
- 云端通信：异步 WebSocket
- 视频渲染：独立渲染线程
